# ðŸ›ï¸ Retail Databricks ETL Pipeline

## Overview
This project demonstrates an **end-to-end ETL pipeline** using **Databricks + PySpark** on a retail dataset (*Sample Superstore*).
It follows the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) and shows how raw sales data becomes business-ready analytics.

## Architecture
- **Bronze** â†’ Ingest raw retail CSV into Delta Lake.
- **Silver** â†’ Clean/standardize (dates, null handling, renaming) and store in Delta (partitioned).
- **Gold** â†’ Aggregate sales & profit by Category and Region for BI.
- **Visualization** â†’ Create charts from Gold tables and save PNGs to DBFS.

## Repository Structure
```
RETAIL-DATABRICKS-ETL/
â”œâ”€â”€ Codes/
â”‚   â”œâ”€â”€ 01_ETL_Operations.py                  # Bronze & Silver ingestion/cleaning
â”‚   â”œâ”€â”€ 02_Delta_Lake_For_Storage.py          # Partitioned Silver storage (Delta)
â”‚   â”œâ”€â”€ 03_Spark_SQL_for_DataTransformations.py  # Gold aggregations
â”‚   â”œâ”€â”€ 04_Visualization_of_Transformed_Data.py  # Charts (sales/profit)
â”‚   â”œâ”€â”€ main.py                               # Entry stub (instructions / orchestration)
â”‚   â””â”€â”€ Test_main.py                          # Repo layout sanity test (pytest)
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ Sample - Superstore.csv               # Retail dataset (upload into DBFS for CE)
â”œâ”€â”€ Resources/                                # Images used in README/demos
â”‚   â”œâ”€â”€ 1116_ETL_Operations - Databricks.png
â”‚   â”œâ”€â”€ 1116_Delta_Lake_For_Storage - Databricks.png
â”‚   â”œâ”€â”€ 1116_Spark_SQL_for_DataTransformations - Databricks.png
â”‚   â”œâ”€â”€ 1116_Automated Workflow - Databricks.png
â”‚   â””â”€â”€ 1116_Automated Workflow_Chart - Databricks.png
â”œâ”€â”€ .github/workflows/                        # (Optional) CI pipelines
â”‚   â”œâ”€â”€ 01_Install.yml
â”‚   â”œâ”€â”€ 02_Format.yml
â”‚   â”œâ”€â”€ 03_Lint.yml
â”‚   â””â”€â”€ 04_Test.yml
â”œâ”€â”€ .devcontainer/                            # (Optional) VS Code dev container
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                                  # Helper targets (install/format/lint/test)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> **Note:** `.github/` and `.devcontainer/` are optional. Keep them if you want CI and containerized dev; they are not required to run the pipeline on Databricks.

## How to Run (Databricks Community Edition or Azure Databricks)
1. **Upload data**: In the UI, go to **Data â†’ Add data â†’ Upload** and upload `Sample - Superstore.csv`.
   - Path will be: `/FileStore/tables/superstore.csv` (or rename in the UI to match).
2. **Import code**:
   - Use **Repos â†’ Add Repo** to connect this GitHub repo, **or**
   - **Workspace â†’ Import** each file from `Codes/` as a notebook.
3. **Attach a cluster** and run in order:
   - `01_ETL_Operations.py` â†’ creates `bronze.superstore`, `silver.superstore`
   - `02_Delta_Lake_For_Storage.py` â†’ creates `silver.superstore_partitioned`
   - `03_Spark_SQL_for_DataTransformations.py` â†’ creates `gold.sales_summary`
   - `04_Visualization_of_Transformed_Data.py` â†’ saves charts to `/dbfs/tmp/`
4. **View charts** (inside a notebook):
   - `/files/tmp/total_sales_by_category.png`
   - `/files/tmp/total_profit_by_region.png`

## Example SQL (Gold)
```sql
SELECT Category, Region, total_sales, total_profit
FROM gold.sales_summary
ORDER BY total_sales DESC;
```

## Development Guide (Makefile + CI)
For local development, use the provided **Makefile** and **requirements.txt**.

### 1) Create a virtual environment (optional but recommended)
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate
```

### 2) Install dependencies
```bash
make install
```

### 3) Format, lint, and test
```bash
make format
make lint
make test
```

The GitHub Actions workflows in `.github/workflows/` run similar steps automatically on every push/PR.

## Requirements (local dev/testing)
If youâ€™re running tests/linters locally:
```bash
pip install -r requirements.txt
pytest
```

## Extensions (Future Work)
- Switch Bronze ingest to **Auto Loader** (streaming file ingestion).
- Add **DLT expectations** and **Unity Catalog** (governance).
- Orchestrate with **Jobs** / **ADF**; add **alerts**.
- Add **MLflow** forecasting on the Gold data.

---

**Author:** Wael Rahhal â€” Data Scientist / Data Engineering
